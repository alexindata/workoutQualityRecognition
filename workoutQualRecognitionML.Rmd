---
title: "Workout quality recognition by machine learning on fitness sensor data"
author: "Wei Wei"
date: "June 25, 2016"
output: 
  html_document: 
    keep_md: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

Using personal fitness trackers collecting large amounts of sensor data, it has been possible to quantify how much a person carries out a particular activity. But rarely has it been addressed how _well_ the person carries out that particular activity. The goal of this project is to use the Weight Lifting Exercise Dataset, kindly provided by groupware@les, to correctly predict how _well_ a person lifts weights.

The dataset was collected from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways, classified as "A" to "E". Read more [here][1]. Using this dataset, a machine learning model was trained and selected based on accuracy performance. The model was then used to predict the workout quality classifications on 20 different test cases. 

[1]: http://groupware.les.inf.puc-rio.br/har#ixzz4CNLAdGub "here"

## HAR data processing and exploratory analysis

#### Basic data structure

The HAR Weight Lifting Exercises Datasets were downloaded and read into Rstudio as "data" and "dataToFit" data frame objects using the read.csv function. "data" contains 19622 observations on 160 variables; while "dataToFit" contains only 20 observations. 

The 1st variable, "X", contains sequentially incrementing integers that are the indexes of each observations. Names of users, timestamp, and sliding-window information are found in the 2nd to the 7th variables. Data collected from the wearable accelerometers are in the 8th to the 159th variables. The 160th variable, "classe", contains the workout classification data with the values of "A" to "E".

"dataToFit" has all the same variables as "data", except that the 160th variable is "problem_id" instead of "classe". The goal of this report is to train a suitable machine learning model and use the selected model to predict the workout quality for the 20 observations in the "dataToFit" dataset.

#### Data reduction and processing

Firstly, it is found that in "data" 67 out of 160 variables have missing data (NAs), and each of these 100 variables contains 19216 NAs out of total 19622 observations; while in "dataToFit" 100 out 160 variables have NAs, and each of these 100 variables contains only NAs. Variables that contain NAs in either "data" or "dataToFit" were excluded from downstream analysis. 

Secondly, the 2nd to the 7th variables were removed since the sensor data should be user-agnostic, and timestamp, number of sliding-windows information should not be relevant to workout quality prediction.

Thirdly, the presence of near zero variance variables were checked and none were found. Therefore, all the remaining variables were kept.

Lastly, the "classe" values in "data" were found to be grouped (see figure below). To ensure randomness in the "classe" values in the dataset, the row order of "data" was scrambled with the seed set at "1234".

The R code chunk and output for data downloading, processing, and analysis are below.

```{r data download & exploratory analysis, message=FALSE, cache=TRUE}

# download data
rm(list=ls())
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urldataToFit <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!dir.exists("../webData")) {dir.create("../webData")}
if(!file.exists("../webData/HARtrain.csv")) 
      {download.file(urlTrain, destfile="../webData/HARtrain.csv", method="curl" )}
if(!file.exists("../webData/HARdataToFit.csv")) 
      {download.file(urldataToFit, destfile="../webData/HARdataToFit.csv", method="curl")}
data <- read.csv("../webData/HARtrain.csv")
dataToFit <- read.csv("../webData/HARdataToFit.csv")


# basic information on the 2 datasets
dim(data); dim(dataToFit)
head(names(data), 9L); tail(names(data), 3L); tail(names(dataToFit), 3L)

# count number of variables that contain missing data
table(colSums(is.na(data))); table(colSums(is.na(dataToFit)))

# select only variables that do not contain missing data in either datasets
data <- subset(data, select=colSums(is.na(data)) == 0 & colSums(is.na(dataToFit)) == 0)
dataToFit <- subset(dataToFit, select=names(dataToFit) %in% c(names(data), "problem_id"))

# exclude time-stamp and sliding-window related variables
data <- data[, -c(2:7)]
dataToFit <- dataToFit[, -c(2:7)]

# flag near zero variance variables
suppressMessages(library(caret, quietly=TRUE, warn.conflicts=FALSE))
length(nearZeroVar(data, saveMetrics = FALSE, names = TRUE))

dim(data); dim(dataToFit)


# exploratory data analysis
table(data$classe)
library(lattice)
xyplot(classe ~ 1:dim(data)[1], data, xlab="row number", ylab="Workout quality (classe)")

# scramble row order in data
set.seed(1234)
data <- data[sample(nrow(data)), ]
```

## Machine learning model training and selection

Since random forests and boosting are among the most common and better performing machine learning algorithms in use, random forests (rf) and generalized boosting model (gbm) were selected for model training using the caret package. Accuracy was set as the metrics for model performance evaluations, since it is an appropriate metrics for factor variables, like "classe". 

The "data" dataset was split in 70/30 ratio into a "training" and a "testing" dataset. The "training" dataset was used to train both a random forest model and a generalized boosting model, both using 10-fold cross-validation.

The random forest model (rfMod) outperformed the generalized booting model (gbmMod) on in-sample accuracy, 0.9923560 vs 0.9622918, and was thus selected. 

The R code chunk and output for machine learning model training and selection are below.


```{r model fitting with parallel processing, message=FALSE, cache=TRUE}
# split tidy data into training and testing datasets in 70/30 portions
# using random data sampling createDataPartition function from the caret package
suppressMessages(library(caret, quietly=TRUE, warn.conflicts=FALSE))
suppressMessages(library(randomForest, quietly=TRUE, warn.conflicts=FALSE))
suppressMessages(library(gbm, quietly=TRUE, warn.conflicts=FALSE))


set.seed(1234)
inTrain <- createDataPartition(y=data$classe, p=0.7, list=FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]


# model training on training dataset
library(parallel, quietly=TRUE, warn.conflicts=FALSE)
library(doParallel, quietly=TRUE, warn.conflicts=FALSE)
library(plyr, quietly=TRUE, warn.conflicts=FALSE)

cluster <- makeCluster(detectCores() - 1) # 1 core for the OS
registerDoParallel(cluster)
trControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)

# 1st model: random forest
rfMod <- train(classe ~., method="rf", data=training[, -1], trControl=trControl, metric="Accuracy")
rfMod$results ## Accuracy : 0.9923560

# 2nd model: generailized boosting regression
gbmMod <- train(classe ~., method="gbm", data=training[, -1], trControl=trControl, metric="Accuracy", verbose=F)
gbmMod$results[, 4:8] ## Accuracy : 0.9622918
```


## Model performance (out-of-sample accuracy) evaluation on "testing" dataset

The selected random forests model (rfMod) was applied to the hold-out samples of the "testing" dataset. The out-of-sample accuracy was found to be 0.9937. 

With a high in-sample accuracy of 0.9923560, it was a concern that the random forests model may have overfitted the "training"" dataset. When models overfit, it is expected to observe the out-of-sample error rate larger than the in-sample error rate. Since the out-of-sample accuracy of 0.9937 is actually better than the in-sample accuracy of 0.9923560, the random forests model did not overfit the "training" dataset.

The top-10 most important variables were identified in the random forests model. It was found that 3 were from the belt sensor, 3 from the forearm sensor, and 4 from the dumbbell.

The R code chunk and output for model performance evaluation are below.

```{r evaluate out-of-sample performance, cache=TRUE}
# select and fit the best model on testing dataset to evaluate out-of-sample accuracy
predTest <- predict(rfMod, newdata=testing)
confusionMatrix(predTest, testing$classe) # Accuracy : 0.9937

# identify the top-10 most important variables in the random forest model
imp <- varImp(rfMod)$importance
newImp <- data.frame(variable=rownames(imp), importance=imp$Overall)
newImp <- newImp[order(newImp$importance, decreasing=TRUE), ]
rownames(newImp) <- 1:dim(newImp)[1]
head(newImp, 10L)
```

## Prediction of workout quality using the selected machine learning model

The random forests model (rfMod) was used to predict the workout quality classifications of the 20 cases in "dataToFit". With the out-of-sample accuracy of 0.9937, the probability of correctly predicting all 20 cases is 0.88.

The R code chunk and output for prediction are below.

``` {r predict workout performance, cache=TRUE}
# fit model on dataToFit dataset to predict/recognize workout quality
rfDataToFit <- predict(rfMod, newdata=dataToFit)

# probability of predicting all 20 workout quality correctly 
# using the rfMod with the out-of-sample accuracy of 0.9937
round(0.9937^20, 2)

stopCluster(cluster)
```




